# -*- coding: utf-8 -*-
"""Dicoding Submission_Image Classification Model Development_Benjamin Nikholas

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ht08l0Dv7KZmhRdFkcvruYv-ZWCuLOEp

# Project Akhir: Image Classification Model Deployment
---
---
* Nama: Benjamin Nikholas
* Email: benjisturi@gmail.com
* Nomor Telp : [6287892677303](wa.me/6287892677303)

Kriteria Parameter:
1. Dataset yang akan dipakai bebas, namun minimal memiliki 1000 buah gambar.
2. Dataset tidak pernah digunakan pada submission kelas machine learning sebelumnya.
3. Dataset dibagi menjadi 80% train set dan 20% test set.
4. Model harus menggunakan model sequential.
5. Model harus menggunakan **Conv2D Maxpooling Layer**.
6. Akurasi pada training dan validation set minimal sebesar 80%.
7. Menggunakan Callback.
8. Membuat plot terhadap akurasi dan loss model.
9. Menulis kode untuk menyimpan model ke dalam format **TF-Lite**.

---

Kriteria target nilai sempurna **(bintang 5)**:
1. dataset yang digunakan memiliki minimal 10000 gambar
2. akurasi pada training set dan validation set minimal 92%
3. memiliki minimal 3 kelas

### Library and Packages
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import zipfile
from google.colab import files
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize'] = 15, 6
import matplotlib.image as mpimg
import seaborn as sns
import tensorflow as tf
from keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
import warnings
warnings.filterwarnings("ignore")
import random
from sklearn.utils import check_random_state
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import shutil

def SetSeed(seed:int):
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)
    check_random_state(seed)

"""### Data Download
---
Data yang digunakan merupakan data image nasi yang didownload dari [kaggle](https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset)


Link lengkap ada dibawah ini:

`https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset`

---
"""

# Install API Kaggle in Google Colaboratory
!pip install kaggle

# Upload API JSON File Credential

uploaded = files.upload()

# Move JSON File and giving access

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download Dataset

!kaggle datasets download -d muratkokludataset/rice-image-dataset

# Unzip zip file

with zipfile.ZipFile('/content/rice-image-dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('/content')

# Delete txt file

file_path = os.path.join('/content/Rice_Image_Dataset', 'Rice_Citation_Request.txt')

if os.path.exists(file_path):
  os.remove(file_path)
  print('200 OK')
else:
  print('404 NOT FOUND')

# Check the number of images in each directory

rice_image_dir = np.array(os.listdir('/content/Rice_Image_Dataset'))

for dir in rice_image_dir:
  length_dir = len(os.listdir(f'/content/Rice_Image_Dataset/{dir}'))
  print(f'Total {dir} images: {length_dir}')

# Directory benchmark

rice_image_dir = '/content/Rice_Image_Dataset'
output_dir = '/content/Rice_Image_Dataset_Limited'

# Limit the number of images to 3000 per directory
num_images_per_directory = 3000

# Create a new directory for the limited dataset
os.makedirs(output_dir, exist_ok=True)

for dir_name in os.listdir(rice_image_dir):
    current_dir = os.path.join(rice_image_dir, dir_name)
    output_subdir = os.path.join(output_dir, dir_name)
    os.makedirs(output_subdir, exist_ok = True)

    all_images = os.listdir(current_dir)
    selected_images = all_images[:num_images_per_directory]

    # Copy selected images to the new directory
    for image_name in selected_images:
        source_path = os.path.join(current_dir, image_name)
        destination_path = os.path.join(output_subdir, image_name)
        shutil.copyfile(source_path, destination_path)

# Verify the total number of images

total_images = sum([len(files) for _, _, files in os.walk(output_dir)])
print(f'Total images: {total_images}')

# Create data generators
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# Set image shape
image_shape = (20, 20)

train_generator = datagen.flow_from_directory(
    output_dir,
    target_size = image_shape,
    class_mode = 'categorical',
    subset = 'training'
)

val_generator = datagen.flow_from_directory(
    output_dir,
    target_size = image_shape,
    class_mode = 'categorical',
    subset = 'validation'
)

"""### Exploratory Data Analysis

### CNN Model
"""

# CNN Model

SetSeed(2024)

# Set input image shape on first layer
input_image = (image_shape, 3)
input_image = (*input_image[0], input_image[1])

model = Sequential([

    # Add Convolutional Layer
    layers.Conv2D(64,
                  (3,3),
                  activation = 'relu',
                  input_shape = input_image),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(32,
                  (3,3),
                  activation = 'relu'),
    layers.MaxPooling2D(2, 2),

    layers.Dropout(0.5),

    layers.Flatten(),

    # Add Fully Connected Layer
    layers.Dense(128,
                 activation = 'relu'),
    layers.Dense(64,
                 activation = 'relu'),
    layers.Dense(5,
                 activation = 'softmax')
])

# Implement Callbacks

SetSeed(2024)

EarlyStop = tf.keras.callbacks.EarlyStopping(
    monitor = 'accuracy',
    min_delta = 0.001,
    verbose = 2,
    patience = 2,
    restore_best_weights = True,
    mode = 'max'
)

# Model Compiling

model.compile(loss = 'categorical_crossentropy',
              optimizer = 'adam',
              metrics = 'accuracy')

# Model Fitting

model_fit = model.fit(train_generator,
                      validation_data = val_generator,
                      epochs = 50,
                      verbose = 2,
                      batch_size = 64,
                      callbacks = EarlyStop)

# Check model fitting history data

df_history = pd.DataFrame(model_fit.history)
df_history.head()

# Check accuracy on train and validation data

min_accuracy = 0.92

print(f'train accuracy > 92% is {df_history.accuracy.max() > min_accuracy}')
print(f'validation accuracy > 92% is {df_history.val_accuracy.max() >= min_accuracy}')

# Plot Loss on Training and Validation data

plt.plot(df_history.loss, label = 'train loss')
plt.plot(df_history.val_loss, label = 'val loss')
plt.title('Train and Validation Loss')
plt.legend()

# Plot Accuracy on Train and Validation data

plt.plot(df_history.accuracy, label = 'train accuracy')
plt.plot(df_history.val_accuracy, label = 'val accuracy')
plt.title('Train and Validation Accuracy')
plt.legend()

"""### Model Deployment"""

# Save model

save_path = 'mymodel/'
tf.saved_model.save(model, save_path)

# Convert model format in SavedModel to TensorFlow Lite

converter = tf.lite.TFLiteConverter.from_saved_model('/content/mymodel')
tflite_model = converter.convert()

with tf.io.gfile.GFile('CNNModel_Benjamin.tflite', 'wb') as f:
    f.write(tflite_model)

