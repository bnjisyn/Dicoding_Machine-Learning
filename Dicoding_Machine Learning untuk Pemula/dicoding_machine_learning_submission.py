# -*- coding: utf-8 -*-
"""Dicoding Machine Learning Submission

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10XL2Imdd9YOizrz0yWpRRRW3cQH3DxFb

# Submission Final Project Dicoding Course
### **Course: Belajar Machine Learning untuk Pemula**

---
>> Nama: Benjamin Nikholas Partomuan

Kriteria penilaian:
1.   Dataset yang digunakan harus dari [rockpaperscissors](https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip) atau link ini `https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip`
2.   Dataset dibagi menjadi **train set** dan **validation set**
3.   Ukuran **validation set** harus **40%** dari total dataset (data training memiliki 1314 sampel, dan data validasi sebanyak 874 sampel)
4.   Mengimplementasikan **augmentasi gambar**
5.   Menggunakan image **data generator**
6.   Pelatihan model tidak lebih dari **30 menit**
7.   Program dikerjakan pada **Google Colaboratory**
8.   Akurasi model minimal **85%**
9.   Dapat **memprediksi gambar** yang diunggah ke google colab
10.  Menambahkan **data diri**
---
---
"""

!wget https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip

!unzip /content/rockpaperscissors.zip

get_ipython().system('pip install split_folders')

# Libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib import image as mpimg
plt.rcParams['figure.figsize'] = 22, 5
import seaborn as sns
import os
import splitfolders as sf
import random
from google.colab import files
import time

# Scikit-learn
from sklearn.utils import check_random_state

# Tensorflow
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.preprocessing import image

def SetSeed(seed:int):
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)
    check_random_state(seed)

SetSeed(1337)

sf.ratio('/content/rockpaperscissors/rps-cv-images',
         output = '/content/rockpaperscissors/database',
         ratio = (0.6, 0.4))

db_dir = '/content/rockpaperscissors/database'
train_dir = os.path.join(db_dir, 'train')
val_dir = os.path.join(db_dir, 'val')

train_content = os.listdir(train_dir)
val_content = os.listdir(val_dir)

print(train_content)
print(val_content)

for i in train_content:
  content_length = len(os.listdir(f'/content/rockpaperscissors/{i}'))
  print(f'{i.ljust(10)} : {content_length}')

train_data_length = 0
for i in train_content:
  content_length = len(os.listdir(f'/content/rockpaperscissors/database/train/{i}'))
  train_data_length += content_length

val_data_length = 0
for i in val_content:
  content_length = len(os.listdir(f'/content/rockpaperscissors/database/val/{i}'))
  val_data_length += content_length

print(f'panjang data gambar train      (60%): {train_data_length}')
print(f'panjang data gambar validation (40%): {val_data_length}')

"""
Variabel yang dihasilkan:
train_rock_dir, train_paper_dir, train_scissors_dir
train_rock_imgs, train_paper_imgs, train_scissors_imgs
"""

train_dir_paths, train_imgs_paths = [], []
for name in train_content:
  # train_path_name = f'train_{name}_imgs'
  # train_dir_name = f'train_{name}_dir'
  # locals()[train_dir_name] = os.path.join(train_dir, name)
  # locals()[train_path_name] = os.listdir(locals()[train_dir_name])
  temp_path = os.path.join(train_dir, name)
  temp_imgs = os.listdir(temp_path)

  train_dir_paths.append(temp_path)
  train_imgs_paths.append(temp_imgs)

"""
Variabel yang dihasilkan:
val_rock_dir, val_paper_dir, val_scissors_dir
val_rock_imgs, val_paper_imgs, val_scissors_imgs
"""

val_dir_paths, val_imgs_paths = [], []
for name in val_content:
  # val_path_name = f'val_{name}_imgs'
  # val_dir_name = f'val_{name}_dir'
  # locals()[val_dir_name] = os.path.join(val_dir, name)
  # locals()[val_path_name] = os.listdir(locals()[val_dir_name])
  temp_path = os.path.join(val_dir, name)
  temp_imgs = os.listdir(temp_path)

  val_dir_paths.append(temp_path)
  val_imgs_paths.append(temp_imgs)

zipped_iter = zip(train_dir_paths,
                  train_imgs_paths,
                  ['scissors', 'paper', 'rock'])
for dir_paths, dir_imgs, name in zipped_iter:
  for i, img_path in enumerate(dir_imgs[:5]):
    sp = plt.subplot(1, 5, i+1)
    img = mpimg.imread(os.path.join(dir_paths, img_path))
    plt.imshow(img)
  plt.suptitle(name, fontsize = 20)
  plt.show()
  print(' ')

# Augmentasi data gambar
datagen_train = ImageDataGenerator(rescale=1./255, validation_split=0.9999, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
datagen_val = ImageDataGenerator(rescale=1./255, validation_split=0.9999)

train_generator = datagen_train.flow_from_directory(
    train_dir,
    target_size = (100, 100),
    batch_size = 32,
    class_mode = 'categorical',
    color_mode = 'rgb',
    shuffle = True
)

val_generator = datagen_val.flow_from_directory(
    val_dir,
    target_size = (100, 100),
    batch_size = 32,
    class_mode = 'categorical',
    color_mode = 'rgb',
    shuffle = True,
)

print(train_generator.class_indices)
print(val_generator.class_indices)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# conv_neurons = [16,32, 64, 128, 256]
# 
# model = Sequential([
#     layers.Conv2D(conv_neurons[0], (3, 3), activation = 'relu', input_shape = (100, 100, 3)),
#     layers.Conv2D(conv_neurons[0], (3, 3), activation = 'relu'),
#     layers.MaxPooling2D((2, 2)),
#     layers.Dropout(0.25),
#     layers.Conv2D(conv_neurons[1], (3, 3), activation = 'relu'),
#     # layers.Conv2D(conv_neurons[1], (3, 3), activation = 'relu'),
#     layers.MaxPooling2D((2, 2)),
#     layers.Dropout(0.25),
#     # layers.Conv2D(conv_neurons[2], (3, 3), activation = 'relu'),
#     layers.Conv2D(conv_neurons[2], (3, 3), activation = 'relu'),
#     layers.MaxPooling2D((2, 2)),
#     layers.Dropout(0.25),
#     layers.Conv2D(conv_neurons[3], (3, 3), activation = 'relu'),
#     # layers.Conv2D(conv_neurons[3], (3, 3), activation = 'relu', strides = (1, 1), padding = 'same'),
#     layers.MaxPooling2D((2, 2)),
#     layers.Dropout(0.25),
#     layers.GlobalAveragePooling2D(),
#     layers.Flatten(),
#     layers.Dense(512, activation = 'relu'),
#     layers.Dense(3, activation = 'softmax')
# ])

AMSGrad = optimizers.Adam(amsgrad = True, name = 'Adam')

model.compile(loss = 'categorical_crossentropy',
              metrics = 'accuracy',
              optimizer = AMSGrad)

EarlyStop = tf.keras.callbacks.EarlyStopping(
    monitor = 'val_loss',
    min_delta = 0.001,
    verbose = 1,
    patience = 5,
    restore_best_weights = True,
    mode = 'min'
)

start_time = time.time()

model_fit = model.fit(
    train_generator,
    epochs = 100,
    validation_data = val_generator,
    validation_steps = 5,
    shuffle = False,
    callbacks = EarlyStop,
)

end_time = time.time()
execution_time = end_time - start_time

print(f'\n\nLama pelatihan model:\n{execution_time:.3f} detik atau {execution_time/60:.3f} menit')

df_history = pd.DataFrame(model_fit.history)
print(f'akurasi model = {df_history.accuracy.tail(1).values}')

uploaded = files.upload()

for fn in uploaded.keys():
    path = fn
    img = image.load_img(path, target_size=(100,100))
    imgplot = plt.imshow(img)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)

    images = np.vstack([x])
    classes = model.predict(images, batch_size=32)

    print(fn)
    if classes[0,0] == 1.0:
      print('Paper')
    elif classes[0,1] == 1.0:
      print('Rock')
    else:
      print('Scissors')

